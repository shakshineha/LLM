{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "collapsed": true,
        "id": "Op7-1ZyfbcT6",
        "outputId": "37ba9a2d-3dbf-4558-865b-ac2fd116fe7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp311-cp311-linux_x86_64.whl size=4552822 sha256=e7923fb84af71f6ee2540df1dbf4b8628d4025cb49e8d39e2db9b3bd92a00190\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/82/79/ac77fcd49324b75ae6aa18e63a87cf9da4371a57e2cdc8dc03\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fabb940b7a1b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaCpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/llms/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mllms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;31m# If not in interactive env, raise warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "import os\n",
        "from langchain.llms import LlamaCpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "niG8e3HI28Wt"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcpUwEoOpgtR"
      },
      "outputs": [],
      "source": [
        "!pip install mysql-connector-python sqlalchemy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCNnHqJOGRyi"
      },
      "outputs": [],
      "source": [
        "!pip install tavily-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTsuQTzWZuY_"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3qCLI4SVl_R"
      },
      "source": [
        "# **Add History to Prompt**\n",
        "Process: Collect previous Q&A context and append it to the current user query to enhance the LLM’s understanding for follow-up questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oT02r7bVkYB"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "\n",
        "# Initialize ConversationBufferWindowMemory with a window size (k) of 5 interactions\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=5, return_messages=True)\n",
        "\n",
        "def update_conversation_memory(memory, human_input: str, ai_response: str):\n",
        "    \"\"\"\n",
        "    Updates the ConversationBufferWindowMemory with a new conversation turn.\n",
        "\n",
        "    Args:\n",
        "        memory: The ConversationBufferWindowMemory instance.\n",
        "        human_input: The new user prompt.\n",
        "        ai_response: The AI's response corresponding to the prompt.\n",
        "\n",
        "    Returns:\n",
        "        Updated memory containing the new turn.\n",
        "    \"\"\"\n",
        "    # Add the new user message and AI response to the memory's chat_history.\n",
        "    memory.chat_memory.add_message(HumanMessage(content=human_input))\n",
        "    memory.chat_memory.add_message(AIMessage(content=ai_response))\n",
        "    return memory\n",
        "\n",
        "# Example usage:\n",
        "# Simulate a conversation turn:\n",
        "human_input = \"Hello, how are you today?\"\n",
        "ai_response = \"I'm doing well, thank you! How can I assist you?\"\n",
        "memory = update_conversation_memory(memory, human_input, ai_response)\n",
        "\n",
        "# Retrieve the current conversation history as a string (or list of messages)\n",
        "current_history = memory.load_memory_variables({})[\"chat_history\"]\n",
        "print(\"Current Conversation History:\\n\", current_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xzfn9tYOVwWK"
      },
      "source": [
        "# **Router Decision Process**\n",
        "Process: Analyze the user query to decide whether it should be processed via internal database querying or external web search (e.g., using keywords like “sales” or “disaster”)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A6pbSSnVvrw"
      },
      "outputs": [],
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "openai_api_key = \"\"\n",
        "\n",
        "# Initialize the LLM (using a deterministic setting)\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0,openai_api_key=openai_api_key)\n",
        "\n",
        "def decide_query_type(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Decides whether a query should be processed via an internal SQL query (use_db)\n",
        "    or via external web search (use_web) based on the provided database schema.\n",
        "    \"\"\"\n",
        "    # Define the schema context based on your table schematics:\n",
        "    schema = \"\"\"\n",
        "    Database Schema:\n",
        "    1. Users (Users): Id (PK), Name, Email (UK), PhoneNumber.\n",
        "    2. NGOs (Ngos): ID (PK), Name, Code (UK).\n",
        "    3. Campaigns (Campaigns): ID (PK), Title, NgoID (FK → Ngos(ID)), CampaignerId (FK → Users(Id)), Status.\n",
        "    4. Categories (Category): Id (PK), Name, Priority.\n",
        "    5. Campaign Categories (CampaignCategory): CampaignId (PK, FK → Campaigns(ID)), CategoryId (PK, FK → Category(Id)), Priority.\n",
        "    6. Campaign Orders (CampaignOrders): Id (PK), CampaignId (FK → Campaigns(ID)), TransactionReference (FK → Transactions(Reference)), Amount.\n",
        "    7. Transactions (Transactions): Reference (PK, UK), UserID (FK → Users(Id)), Amount, Status.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a prompt template that instructs the LLM to decide the routing\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"schema\", \"query\"],\n",
        "        template=\"\"\"\n",
        "You are an expert in database querying.\n",
        "Given the following database schema:\n",
        "{schema}\n",
        "\n",
        "And the user query: \"{query}\"\n",
        "\n",
        "Decide whether this query is intended for data retrieval from the database or if it is a general question better answered using external web search.\n",
        "If the query refers to data that exists in the tables (for example, asking for totals, counts, or other metrics related to campaigns, orders, etc.), respond with \"use_db\". Otherwise, respond with \"use_web\".\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Format the prompt with the schema and query\n",
        "    prompt = prompt_template.format(schema=schema, query=query)\n",
        "\n",
        "    # Get the LLM's decision\n",
        "    decision = llm.predict(prompt)\n",
        "\n",
        "    # Clean up the output (expecting a short response \"use_db\" or \"use_web\")\n",
        "    decision = decision.strip().lower()\n",
        "    if \"use_db\" in decision:\n",
        "        return \"use_db\"\n",
        "    else:\n",
        "        return \"use_web\"\n",
        "\n",
        "# Example usage:\n",
        "queries = [\n",
        "    \"How is a llm model?\",\n",
        "    \"What is the total revenue for campaign orders?\",\n",
        "    \"Tell me about the latest global technology news.\",\n",
        "    \"What are the total transactions for the last month?\"\n",
        "]\n",
        "\n",
        "for q in queries:\n",
        "    route = decide_query_type(q)\n",
        "    print(f\"Query: {q}\\nRouted to: {route}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuaSX7JMV5CB"
      },
      "source": [
        "# **Fetch Schema Process**\n",
        "Process: Dynamically extract relevant schema metadata from your MySQL database (via Google Cloud RDS). This includes retrieving table names, columns, and foreign key relationships of all the schemas in a LLM friendly way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVRHZ6llV4YE"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities.sql_database import SQLDatabase\n",
        "from sqlalchemy.sql import text  # Import text for executing raw SQL queries\n",
        "import urllib.parse\n",
        "\n",
        "# ✅ Encode password correctly for MySQL connection\n",
        "user = \"aadish\"\n",
        "password = urllib.parse.quote(\"Puiya@4369\")  # Encodes special characters\n",
        "host = \"34.59.101.173\"\n",
        "port = \"3306\"\n",
        "database = \"test_schema\"\n",
        "\n",
        "# ✅ Create the database connection\n",
        "db = SQLDatabase.from_uri(f\"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}\")\n",
        "\n",
        "def fetch_schema_metadata():\n",
        "    \"\"\"\n",
        "    Dynamically fetches schema metadata including:\n",
        "    - Table names\n",
        "    - Column details\n",
        "    - Foreign key relationships\n",
        "    \"\"\"\n",
        "    # ✅ Retrieve table information\n",
        "    table_info = db.get_table_info()\n",
        "\n",
        "    # ✅ Query to get foreign key relationships\n",
        "    foreign_key_query = text(\"\"\"\n",
        "        SELECT\n",
        "            TABLE_NAME, COLUMN_NAME, CONSTRAINT_NAME, REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME\n",
        "        FROM\n",
        "            INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
        "        WHERE\n",
        "            TABLE_SCHEMA = :database AND REFERENCED_TABLE_NAME IS NOT NULL;\n",
        "    \"\"\")\n",
        "\n",
        "    # ✅ Execute query to get foreign key relationships\n",
        "    with db._engine.connect() as connection:\n",
        "        result = connection.execute(foreign_key_query, {\"database\": database})\n",
        "        foreign_key_relations = result.fetchall()\n",
        "\n",
        "    # ✅ Structure foreign key relationships\n",
        "    foreign_keys = []\n",
        "    for row in foreign_key_relations:\n",
        "        foreign_keys.append({\n",
        "            \"table\": row[0],\n",
        "            \"column\": row[1],\n",
        "            \"constraint\": row[2],\n",
        "            \"referenced_table\": row[3],\n",
        "            \"referenced_column\": row[4]\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"tables\": table_info,\n",
        "        \"foreign_keys\": foreign_keys\n",
        "    }\n",
        "\n",
        "# ✅ Example Usage\n",
        "schema_metadata = fetch_schema_metadata()\n",
        "print(schema_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be6m6UIwWB2R"
      },
      "source": [
        "# **Generate SQL Context for efficient Query (RAG-based)**\n",
        "Process: Using the retrieved schema  and the NLP query to return Effiient SQL context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72HqAsXz0eYB"
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from typing import List, Dict\n",
        "\n",
        "class TableDetailsSchema(BaseModel):\n",
        "    Tables: List[str]\n",
        "    Attributes: Dict[str, List[str]]\n",
        "    FK_Relationships: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNGSo_Dq0iRu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Optional, Set\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Define the schema using Pydantic.\n",
        "class TableDetailsSchema(BaseModel):\n",
        "    \"\"\"Schema for required tables, attributes, and relationships.\"\"\"\n",
        "    tables_required: List[str] = Field(..., description=\"List of required table names\")\n",
        "    attributes_required: Optional[Dict[str, Set[str]]] = Field(\n",
        "        None,\n",
        "        description=(\n",
        "            \"Dictionary mapping table names to required attributes. \"\n",
        "            \"For example: {'users': {'id', 'name'}}\"\n",
        "        )\n",
        "    )\n",
        "    relationships: List[str] = Field(..., description=\"List of foreign key relationships\")\n",
        "\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0, openai_api_key=openai_api_key)\n",
        "structured_llm = llm.with_structured_output(TableDetailsSchema, method=\"function_calling\")\n",
        "\n",
        "def get_table_details(nl_query: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Uses the schema metadata and the natural language query to return a structured JSON output.\n",
        "\n",
        "    Args:\n",
        "        nl_query (str): The natural language query describing the data requirements.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured response listing required tables, attributes, and relationships.\n",
        "    \"\"\"\n",
        "\n",
        "    schema_metadata = fetch_schema_metadata()\n",
        "\n",
        "    input_data = (\n",
        "        f\"Database Schema:\\n\\nTables Definition:\\n{schema_metadata['tables']}\\n\\n\"\n",
        "        f\"Foreign Keys:\\n{schema_metadata['foreign_keys']}\\n\\n\"\n",
        "        f\"User Query:\\n{nl_query}\"\n",
        "    )\n",
        "    structured_output = structured_llm.invoke(input_data)\n",
        "\n",
        "    result_dict = structured_output.model_dump()\n",
        "\n",
        "    return result_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DV57NJ8r3FQp"
      },
      "outputs": [],
      "source": [
        "nl_query = \"which was the highest transaction users\"\n",
        "result = get_table_details(nl_query)\n",
        "print(\"Structured Output:\\n\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZkiOzNIuYOB"
      },
      "source": [
        "# **  Generate SQL QUERY**\n",
        "Generate a sql query using the given context and the nlp query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-d_N17eumn2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Define the schema for SQL query structured output.\n",
        "class SQLQuerySchema(BaseModel):\n",
        "    query: str = Field(..., description=\"The generated SQL query\")\n",
        "\n",
        "# Initialize the LLM and structured output for both table details and SQL query.\n",
        "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0, openai_api_key=openai_api_key)\n",
        "\n",
        "# Assuming get_table_details is already defined and returns a dictionary with keys \"tables\", \"foreign_keys\", etc.\n",
        "# For the SQL query, create a structured output version using our SQLQuerySchema.\n",
        "structured_sql_llm = llm.with_structured_output(SQLQuerySchema, method=\"function_calling\")\n",
        "\n",
        "# EOS_TOKEN to mark the end of the prompt output.\n",
        "EOS_TOKEN = \"<EOS>\"\n",
        "\n",
        "def format_sql_prompt(instruction: str, input_data: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a standardized prompt with EOS_TOKEN that instructs the LLM to return\n",
        "    only a valid SQL query based on the provided context (schema metadata) and natural language query.\n",
        "    \"\"\"\n",
        "    sql_prompt = (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"{instruction}\\n\\n\"\n",
        "        \"### Input:\\n\"\n",
        "        \"{input}\\n\\n\"\n",
        "        \"### Output (ONLY SQL query):\\n\\n\"\n",
        "        \"Do NOT provide any explanation or additional text. Return only a valid SQL query \"\n",
        "        \"that retrieves the answer based on the schema provided.\"\n",
        "    )\n",
        "    return sql_prompt.format(instruction=instruction, input=input_data) + EOS_TOKEN\n",
        "\n",
        "def generate_sql_query(nl_query: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Uses the retrieved schema metadata (including table definitions and foreign key info)\n",
        "    and the natural language query to prompt an LLM (via a RAG-based approach) to generate an SQL query.\n",
        "\n",
        "    Args:\n",
        "        nl_query (str): The natural language query describing the data requirements.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured response containing the generated SQL query with the key \"query\".\n",
        "    \"\"\"\n",
        "    # Retrieve schema metadata using your existing function.\n",
        "    schema_metadata = get_table_details(nl_query)  # Expected to return a dict with keys like \"tables\" and \"foreign_keys\"\n",
        "    # Extract required details.\n",
        "    tables = schema_metadata.get(\"tables_required\", [])          # List of required tables.\n",
        "    foreign_keys = schema_metadata.get(\"relationships\", [])  # List of foreign key relationships.\n",
        "    attributes = schema_metadata.get(\"attributes_required\", [])    # List of required attributes.\n",
        "\n",
        "    # Build the schema context.\n",
        "    schema_context = (\n",
        "        f\"**Tables Required:** {', '.join(tables)}\\n\\n\"\n",
        "        f\"**Attributes Required:** {', '.join(attributes)}\\n\\n\"\n",
        "        f\"**Foreign Key Relationships:**\\n\" + ('\\n'.join(foreign_keys) if foreign_keys else \"None\")\n",
        "    )\n",
        "\n",
        "\n",
        "    # Combine the schema context with the natural language query.\n",
        "    input_data = f\"{schema_context}\\n\\n**Query:** {nl_query}\"\n",
        "\n",
        "    # Build the SQL prompt with instructions to generate an SQL query.\n",
        "    prompt = format_sql_prompt(\n",
        "        instruction=(\n",
        "            \"Analyze the provided database schema and its foreign key relationships. \"\n",
        "            \"Based on the natural language query, generate a valid SQL query that retrieves the requested data using he provided database schema and its foreign key relationships..\"\n",
        "        ),\n",
        "        input_data=input_data\n",
        "    )\n",
        "\n",
        "    # Get the SQL query from the structured LLM.\n",
        "    structured_output = structured_sql_llm.invoke(prompt)\n",
        "\n",
        "    # Convert the Pydantic object to a dictionary.\n",
        "    result_dict = structured_output.model_dump()\n",
        "    return result_dict['query']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nl_query = \"which was the highest transaction users\"\n",
        "    sql_structured_output = generate_sql_query(nl_query)\n",
        "    print(\"\\nFinal Structured SQL Output:\")\n",
        "    print(sql_structured_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmZffSPsvbTS"
      },
      "outputs": [],
      "source": [
        "# --- Example Usage ---\n",
        "nl_query = \"which was the highest transaction user\"\n",
        "result = generate_sql_query(nl_query)\n",
        "print(\"Generated SQL Query:\\n\")\n",
        "print(sql_structured_output['query'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eG2BCjoWGii"
      },
      "source": [
        "# **NLP Hallucination Check Process**\n",
        "Process: Validate the generated SQL query for potential inaccuracies or hallucinations. This can involve an LLM-based review or rule-based checks before executing the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nMffI0eWLLq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9j7MqwJWQ2_"
      },
      "source": [
        "## **# Execute SQL Process**\n",
        "Process: Run the validated SQL query against your MySQL database instance and retrieve the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rK5B9vyWUA3"
      },
      "outputs": [],
      "source": [
        "# Access the SQL query from the structured output dictionary.\n",
        "sql_query_str = sql_structured_output['query']\n",
        "\n",
        "# Execute the SQL query using the SQLDatabase instance 'db'.\n",
        "result = db.run(sql_query_str)\n",
        "\n",
        "# Print the result of the executed query.\n",
        "print(\"Query Result:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeCmLmswWYNB"
      },
      "source": [
        "# **Formulate SQL-Based Answer Process**\n",
        "Process: Convert the raw SQL query results into a structured, human-readable output (e.g., JSON, tables) and optionally provide a natural language explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuvwsUgHWaFz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def format_sql_answer(sql_result, nl_query):\n",
        "    \"\"\"\n",
        "    Converts raw SQL query results into a structured, human-readable output,\n",
        "    including a dynamic natural language explanation generated by the LLM.\n",
        "\n",
        "    Args:\n",
        "        sql_result (list or dict): The raw result from executing the SQL query.\n",
        "        nl_query (str): The natural language query given as input.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with a dynamic explanation and the structured data.\n",
        "    \"\"\"\n",
        "    # Create a prompt to instruct the LLM to generate a natural language explanation.\n",
        "    prompt = (\n",
        "        f\"Based on the following natural language query and SQL query result, \"\n",
        "        f\"please provide a clear, concise explanation of what the result represents:\\n\\n\"\n",
        "        f\"Natural Language Query: {nl_query}\\n\\n\"\n",
        "        f\"SQL Query Result: {sql_result}\\n\\n\"\n",
        "        f\"Explanation:\"\n",
        "    )\n",
        "\n",
        "    # Use the LLM to generate the explanation dynamically.\n",
        "    explanation = llm.predict(prompt)\n",
        "\n",
        "    # Format the output in a structured dictionary.\n",
        "    formatted_output = {\n",
        "        \"explanation\": explanation,\n",
        "        \"natural_language_query\": nl_query,\n",
        "        \"data\": sql_result\n",
        "    }\n",
        "\n",
        "    return formatted_output\n",
        "\n",
        "# Example usage:\n",
        "# Assume `sql_structured_output` contains your SQL query and you've executed it:\n",
        "sql_query_str = sql_structured_output  # Extract SQL query from the structured output.\n",
        "result = db.run(sql_query_str)  # Execute the SQL query against your database.\n",
        "\n",
        "# Format the SQL result into a human-readable output with dynamic explanation.\n",
        "formatted_answer = format_sql_answer(result, nl_query)\n",
        "\n",
        "# Print the final structured output as formatted JSON.\n",
        "print(json.dumps(formatted_answer, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0ifj_-3WdmF"
      },
      "source": [
        "# **Web Search Process**\n",
        "Process: For queries routed externally, perform a web search using a tool like Tavily. Retrieve relevant news or external data that may answer the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc-5oFe9Wfy9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "from tavily import TavilyClient\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# ---------------------------\n",
        "# Setup Tavily for Web Search\n",
        "# ---------------------------\n",
        "TAVILY_API_KEY = \"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\"\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "def perform_web_search(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Performs a web search using Tavily and returns the structured search results.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query string.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the search query and a list of results.\n",
        "    \"\"\"\n",
        "    search_response = tavily_client.search(query)\n",
        "    structured_results = {\n",
        "        \"query\": query,\n",
        "        \"results\": search_response.get(\"results\", [])\n",
        "    }\n",
        "    return structured_results\n",
        "\n",
        "# --------------------------------------------\n",
        "# Setup LLM for Disaster News Relevance Evaluation\n",
        "# --------------------------------------------\n",
        "class NewsRelevanceSchema(BaseModel):\n",
        "    is_relevant: bool = Field(..., description=\"Whether the news article is relevant to the query\")\n",
        "    authenticated: bool = Field(..., description=\"Whether the news source appears credible and authenticated\")\n",
        "    explanation: str = Field(..., description=\"Explanation for the relevance and authentication assessment\")\n",
        "    additional_info: Optional[str] = Field(None, description=\"Any additional recommendations or extra context\")\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0, openai_api_key=openai_api_key)\n",
        "structured_news_llm = llm.with_structured_output(NewsRelevanceSchema, method=\"function_calling\")\n",
        "\n",
        "def check_news_relevance(sensitive_query: str, news_article: str) -> dict:\n",
        "    \"\"\"\n",
        "    Uses the LLM to evaluate if the provided news article content accurately\n",
        "    reports on current disaster events in India and if the source appears credible.\n",
        "\n",
        "    Args:\n",
        "        sensitive_query (str): The disaster-related query.\n",
        "        news_article (str): The content or snippet from the news article.\n",
        "\n",
        "    Returns:\n",
        "        dict: A structured JSON object with relevance, authentication, and explanation.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are provided with a sensitive query regarding natural disasters in India and a news article summary. \"\n",
        "        \"Please analyze the article and determine if it accurately reports on current disaster events in India, such as floods, landslides, or heatwaves. \"\n",
        "        \"Evaluate whether the article is relevant to the query and if the information is credible and reliable. \"\n",
        "        \"Provide a brief explanation for your assessment. \"\n",
        "        \"Respond with a JSON object containing the following keys:\\n\"\n",
        "        \"  - is_relevant (bool): True if the article is relevant to the query, False otherwise.\\n\"\n",
        "        \"  - authenticated (bool): True if the news source appears credible, False otherwise.\\n\"\n",
        "        \"  - explanation (str): A brief explanation of your reasoning.\\n\"\n",
        "        \"  - additional_info (optional str): Any extra context or recommendations.\\n\\n\"\n",
        "        f\"Sensitive Query: {sensitive_query}\\n\\n\"\n",
        "        f\"News Article Content: {news_article}\\n\\n\"\n",
        "        \"Response:\"\n",
        "    )\n",
        "    structured_output = structured_news_llm.invoke(prompt)\n",
        "    return structured_output.model_dump()\n",
        "\n",
        "# --------------------------------\n",
        "# Example Usage & Final Execution\n",
        "# --------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"Mention all the disasters happening currently in India this month\"\n",
        "    search_results = perform_web_search(query)\n",
        "\n",
        "    if search_results.get(\"results\"):\n",
        "        first_result = search_results[\"results\"][0]\n",
        "        # Extract a snippet or content from the result (adjust the key as per Tavily's API response)\n",
        "        news_article = first_result.get(\"snippet\", first_result.get(\"content\", \"\"))\n",
        "        # Updated sensitive query to focus on current natural disasters\n",
        "        sensitive_query = (\n",
        "            \"What are the latest updates on  disasters currently affecting India, \"\n",
        "            \"including floods, landslides, and heatwaves, train collisons etc?\"\n",
        "        )\n",
        "        relevance_info = check_news_relevance(sensitive_query, news_article)\n",
        "        print(json.dumps(relevance_info, indent=2))\n",
        "    else:\n",
        "        print(\"No search results found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj7H-Gs0ZrOY"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langgraph langsmith\n",
        "!pip install langchain langchain_groq langchain_community\n",
        "\n",
        "# Import necessary modules\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY1ZufuIeav9"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from typing import Optional, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class QueryState(BaseModel):\n",
        "    query: str\n",
        "    route: Optional[str] = None\n",
        "    sql_query: Optional[Dict[str, Any]] = None\n",
        "    sql_output: Optional[str] = None\n",
        "    result: Optional[str] = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKJ2qtO5W-lA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def router(state: QueryState):\n",
        "    print(f\"Routing query: {state.query}\")\n",
        "    state.route = \"use_db\" if \"transaction\" in state.query.lower() else \"use_web\"\n",
        "    print(f\"Route decided: {state.route}\")\n",
        "    return state\n",
        "\n",
        "def use_db(state: QueryState):\n",
        "    print(f\"Fetching table details for query: {state.query}\")\n",
        "    schema_metadata = get_table_details(state.query)\n",
        "    state.sql_query = schema_metadata  # Store the schema details\n",
        "    return state\n",
        "\n",
        "def generate_sql(state: QueryState):\n",
        "    print(f\"Generating SQL query for: {state.query}\")\n",
        "    sql_output = generate_sql_query(state.query)\n",
        "    print(f\"Generated SQL: {sql_output}\")\n",
        "\n",
        "    # Execute the SQL query\n",
        "    try:\n",
        "        db_result = db.run(sql_output)  # Assuming `db.run(query)` executes the SQL\n",
        "        state.sql_output = db_result\n",
        "        print(f\"Query Execution Result: {db_result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing SQL query: {e}\")\n",
        "        state.sql_output = str(e)\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def format_answer(state: QueryState):\n",
        "    print(f\"Formatting answer details for query: {state.query}\")\n",
        "    final_output = format_sql_answer(state.sql_output,state.query)\n",
        "    state.result = final_output  # Store the schema details\n",
        "    print(f\"Formatted Answer: {final_output}\")\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8U4MLIMBebI"
      },
      "outputs": [],
      "source": [
        "graph_builder = StateGraph(QueryState)\n",
        "\n",
        "graph_builder.add_node(\"router\", router)\n",
        "graph_builder.add_node(\"use_db\", use_db)\n",
        "graph_builder.add_node(\"generate_sql\", generate_sql)\n",
        "graph_builder.add_node(\"format_answer\", format_answer)\n",
        "\n",
        "graph_builder.set_entry_point(\"router\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"router\",\n",
        "    lambda state: state.route,\n",
        "    {\n",
        "        \"use_db\": \"use_db\",\n",
        "        \"use_web\": END,\n",
        "    },\n",
        ")\n",
        "graph_builder.add_edge(\"use_db\", \"generate_sql\")\n",
        "graph_builder.add_edge(\"generate_sql\", \"format_answer\")\n",
        "graph_builder.set_finish_point(\"format_answer\")\n",
        "\n",
        "# Compile the graph\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qspHQKUAExTr"
      },
      "outputs": [],
      "source": [
        "state = QueryState(query=\"which are llms\")\n",
        "output = graph.invoke(state)\n",
        "\n",
        "# Print final output\n",
        "print(\"Final Output:\", output['result'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja13ZZgbEjIF"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# Generate and display the graph\n",
        "try:\n",
        "    graph_image = graph.get_graph().draw_mermaid_png()\n",
        "    display(Image(graph_image))\n",
        "except Exception as e:\n",
        "    print(f\"Error generating graph: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
